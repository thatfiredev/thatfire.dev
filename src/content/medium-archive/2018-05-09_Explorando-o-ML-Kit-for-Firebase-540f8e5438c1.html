<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Explorando o ML Kit for Firebase</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Explorando o ML Kit for Firebase</h1>
</header>
<section data-field="subtitle" class="p-summary">
O novo servi√ßo de Machine Learning do Firebase
</section>
<section data-field="body" class="e-content">
<section name="298b" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="77ff" id="77ff" class="graf graf--figure graf--leading"><img class="graf-image" data-image-id="1*eDt4SQ6HqFo6pUG8CSzXiA.png" data-width="1043" data-height="313" src="https://cdn-images-1.medium.com/max/800/1*eDt4SQ6HqFo6pUG8CSzXiA.png"></figure><h3 name="afd1" id="afd1" class="graf graf--h3 graf-after--figure graf--title">Explorando o ML Kit for¬†Firebase</h3><h4 name="6983" id="6983" class="graf graf--h4 graf-after--h3 graf--subtitle">O novo servi√ßo de Machine Learning do¬†Firebase</h4><p name="c42c" id="c42c" class="graf graf--p graf-after--h4">Este artigo faz parte da s√©rie Explorando o ML Kit for Firebase:</p><ol class="postList"><li name="e33c" id="e33c" class="graf graf--li graf-after--p">O novo servi√ßo de Machine Learning do Firebase</li><li name="c66f" id="c66f" class="graf graf--li graf-after--li graf--trailing"><a href="https://medium.com/android-dev-moz/mlkit-537775bd7052" data-href="https://medium.com/android-dev-moz/mlkit-537775bd7052" class="markup--anchor markup--li-anchor" target="_blank">Linguagem Natural e Identifica√ß√£o de Linguagem</a></li></ol></div></div></section><section name="3bbb" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="881d" id="881d" class="graf graf--p graf--leading">Durante a sess√£o de abertura do <a href="https://events.google.com/io" data-href="https://events.google.com/io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google I/O 2018</a>, que aconteceu ontem(8 de Maio), a Google apresentou algumas novidades para o Firebase: melhorias no <a href="https://firebase.google.com/docs/ab-testing/" data-href="https://firebase.google.com/docs/ab-testing/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A/B Testing</a>, suporte para <a href="https://firebase.google.com/docs/test-lab/ios/firebase-console" data-href="https://firebase.google.com/docs/test-lab/ios/firebase-console" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">iOS no Test Lab</a>, a vers√£o est√°vel do <a href="https://firebase.google.com/docs/perf-mon/" data-href="https://firebase.google.com/docs/perf-mon/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Performance Monitor</a> e a vers√£o <em class="markup--em markup--p-em">preview</em> do <a href="https://firebase.google.com/docs/ml-kit/" data-href="https://firebase.google.com/docs/ml-kit/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ML Kit for Firebase</a>.</p><p name="b57a" id="b57a" class="graf graf--p graf-after--p">De acordo com a documenta√ß√£o, o <a href="https://firebase.google.com/products/ml-kit/" data-href="https://firebase.google.com/products/ml-kit/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ML Kit</a> for Firebase √©:</p><blockquote name="bebb" id="bebb" class="graf graf--blockquote graf-after--p">um SDK para dispositivos m√≥veis que leva Machine Learning da Google para aplicativos Android e iOS em um pacote poderoso e f√°cil de usar.</blockquote><p name="80f5" id="80f5" class="graf graf--p graf-after--blockquote">Voc√™ provavelmente j√° esteve numa situa√ß√£o em que precisava de implementar alguma forma de Intelig√™ncia Artificial ou Machine Learning na sua aplica√ß√£o para tornar ela mais ‚Äúinteligente‚Äù e melhorar a experi√™ncia do seu utilizador. Nessa situa√ß√£o, voc√™ provavelmente n√£o conseguiu come√ßar a utilizar Machine Learning por ser complicado de aprender, ligeiramente mais caro ou talvez porque voc√™ n√£o queria hospedar um modelo ML em um outro servi√ßo.<br>O ML Kit foi introduzido para resolver este problema. Com o ML Kit, voc√™ pode utilizar servi√ßos de Machine Learning com a mesma simplicidade do Firebase.</p><p name="1ff0" id="1ff0" class="graf graf--p graf-after--p">ML Kit for Firebase traz-nos funcionalidades como:</p><ul class="postList"><li name="5bfe" id="5bfe" class="graf graf--li graf-after--p">Legendas para imagens;</li><li name="cf0d" id="cf0d" class="graf graf--li graf-after--li">Reconhecimento de texto;</li><li name="4a1b" id="4a1b" class="graf graf--li graf-after--li">Dete√ß√£o facial;</li><li name="7aa1" id="7aa1" class="graf graf--li graf-after--li">Scan de c√≥digo de barras;</li><li name="246a" id="246a" class="graf graf--li graf-after--li">Dete√ß√£o de pontos de refer√™ncia;</li><li name="1866" id="1866" class="graf graf--li graf-after--li">E respostas inteligentes? ü§î</li></ul><p name="ab92" id="ab92" class="graf graf--p graf-after--li">Fiquei bastante entusiasmado quando eles apresentaram esta funcionalidade que combina <a href="https://cloud.google.com/vision/?authuser=0" data-href="https://cloud.google.com/vision/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google Cloud Vision API</a>, <a href="https://www.tensorflow.org/mobile/tflite/?authuser=0" data-href="https://www.tensorflow.org/mobile/tflite/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow Lite</a>, e <a href="https://developer.android.com/ndk/guides/neuralnetworks/?authuser=0" data-href="https://developer.android.com/ndk/guides/neuralnetworks/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Android Neural Networks API</a>. Por isso, decidi experimentar na hora e partilhar a minha experi√™ncia.</p><h3 name="68ab" id="68ab" class="graf graf--h3 graf-after--p">On-device vs.¬†Cloud</h3><p name="22a4" id="22a4" class="graf graf--p graf-after--h3">Antes de come√ßar a falar sobre as tecnologias, gostaria de explicar a diferen√ßa entre on-device e cloud.</p><ul class="postList"><li name="192b" id="192b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">on-device</strong> significa que a funcionalidade est√° dispon√≠vel no dispositivo, e n√£o precisa de uma conex√£o √† internet para funcionar. No ML Kit, estas funcionalidades provavelmente s√£o fornecidas atrav√©s do TensorFlow Lite e Android Neural Networks API.<br>As tecnologias on-device do ML Kit s√£o: Reconhecimento de Texto, dete√ß√£o facial, scan de c√≥digo de barras e legendas para imagens.</li><li name="7df1" id="7df1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">cloud</strong> significa que a funcionalidade s√≥ est√° dispon√≠vel na cloud. O que significa que o seu dispositivo m√≥vel far√° uma requisi√ß√£o √† cloud e tem de esperar por uma resposta (atrav√©s da internet, obviamente). O ML Kit utiliza a Cloud Vision API para estas funcionalidades. √â importante notar que esta API precisa que voc√™ registre uma forma de pagamento para poder us√°-la. No Firebase, isto significa fazer upgrade para o plano Blaze.<br>As tecnologias cloud do ML Kit s√£o: Reconhecimento de Texto, Legendas para imagens e Detec√ß√£o de pontos de refer√™ncia.</li></ul><p name="758b" id="758b" class="graf graf--p graf-after--li">Se voc√™ estiver no plano Sparkle do Firebase, voc√™ s√≥ tem acesso √†s funcionalidades on-device do ML Kit, como mostra a <a href="https://firebase.google.com/pricing/?hl=pt-pt" data-href="https://firebase.google.com/pricing/?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tabela de pre√ßos do Firebase</a>. Para utilizar as funcionalidades cloud, voc√™ ter√° de passar para o plano Blaze, como mencionei anteriormente.<br>Neste artigo irei demonstrar apenas as funcionalidades on-device.</p><p name="b2d6" id="b2d6" class="graf graf--p graf-after--p graf--trailing">Vou lembrar-lhe que esta funcionalidade ainda est√° na vers√£o <em class="markup--em markup--p-em">‚Äúpreview‚Äù</em>, o que significa que s√≥ pode ser utilizada para testes. <strong class="markup--strong markup--p-strong">N√£o utilize em aplica√ß√µes que se encontram em produ√ß√£o.</strong></p></div></div></section><section name="7bdc" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="65d6" id="65d6" class="graf graf--h3 graf--leading">ML Vision</h3><p name="7b86" id="7b86" class="graf graf--p graf-after--h3">Vision √© o novo SDK do Firebase que permite que voc√™ use os pacotes de intelig√™ncia artificial que fazem o uso da c√¢mera do dispositivo. Todas as funcionalidades que mencionei acima est√£o inclusas neste pacote.</p><p name="07d8" id="07d8" class="graf graf--p graf-after--p">Para poder ter estas funcionalidades na sua app, voc√™ precisa de:</p><p name="981f" id="981f" class="graf graf--p graf-after--p">1. <a href="https://firebase.google.com/docs/android/setup?hl=pt-pt" data-href="https://firebase.google.com/docs/android/setup?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Adicionar o Firebase √† app</a> e de seguida adicionar a depend√™ncia do ML Vision:</p><pre name="2b4d" id="2b4d" class="graf graf--pre graf-after--p">implementation &#39;com.google.firebase:firebase-ml-vision:15.0.0&#39;</pre><p name="8b99" id="8b99" class="graf graf--p graf-after--pre">2. (opcional) Adicionar o modelo ML √† sua app. Basta colocar a seguinte linha ao AndroidManifest.xml:</p><pre name="315a" id="315a" class="graf graf--pre graf-after--p">&lt;meta-data<br>    android:name=&quot;com.google.firebase.ml.vision.DEPENDENCIES&quot;<br>    android:value=&quot;text&quot; /&gt;</pre><p name="4c8d" id="4c8d" class="graf graf--p graf-after--pre">Isto far√° com que a app baixe o modelo ML quando for instalada pela Google Playstore.</p><p name="6c3d" id="6c3d" class="graf graf--p graf-after--p">O valor ‚Äútext‚Äù serve para a funcionalidade de reconhecimento de texto. Se voc√™ quiser adicionar outra funcionalidade, pode coloc√°-las separadas por v√≠rgula, por exemplo:</p><pre name="531f" id="531f" class="graf graf--pre graf-after--p">android:value=&quot;text,barcode,face,label&quot;</pre><p name="78c4" id="78c4" class="graf graf--p graf-after--pre">3. Criar uma vari√°vel do tipo <code class="markup--code markup--p-code">FirebaseVisionImage</code>. Para instanci√°-la, voc√™ precisa passar por par√¢metro um <code class="markup--code markup--p-code">Bitmap</code>, <code class="markup--code markup--p-code">Image</code>, <code class="markup--code markup--p-code">ByteBuffer</code> (ou array de <code class="markup--code markup--p-code">byte</code>) ou um <code class="markup--code markup--p-code">Uri</code> que contem o caminho da imagem.<br>Note que independentemente do par√¢metro que voc√™ passa, √© importante que a imagem esteja ‚Äúde p√©‚Äù, ou seja na rota√ß√£o correta, sen√£o o Firebase n√£o consegue reconhece-la.</p><p name="cc74" id="cc74" class="graf graf--p graf-after--p">Para experimentar o ML Kit eu instanciei a classe <code class="markup--code markup--p-code">FirebaseVisionImage</code> com uma <code class="markup--code markup--p-code">Uri</code>. Estou a utilizar imagens da galeria por ser a forma mais simples de usar o ML Kit.</p><h4 name="5a14" id="5a14" class="graf graf--h4 graf-after--p">Reconhecimento de¬†Texto</h4><p name="f371" id="f371" class="graf graf--p graf-after--h4">Esta funcionalidade permite que o dispositivo do utilizador leia texto (em um idioma baseado no latim) em tempo real, atrav√©s da c√¢mara.</p><p name="55c5" id="55c5" class="graf graf--p graf-after--p">Para utilizar o Reconhecimento de Texto, voc√™ precisa de utilizar o <code class="markup--code markup--p-code">FirebaseVisionTextDetector</code>:</p><pre name="477e" id="477e" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionTextDetector detector = FirebaseVision.getInstance()<br>    .getVisionTextDetector();</code></pre><p name="709a" id="709a" class="graf graf--p graf-after--pre">De seguida, chame o m√©todo <code class="markup--code markup--p-code">detectInImage()</code>, passando a nossa FirebaseVisionImage e adicione um listener. Nesse listener voc√™ poder√° obter os blocos de texto que foram lidos da imagem. Veja o c√≥digo completo no fim do artigo.</p><h4 name="d2de" id="d2de" class="graf graf--h4 graf-after--p">Scan de c√≥digo de¬†barras</h4><p name="12e0" id="12e0" class="graf graf--p graf-after--h4">Tal como o nome indica, esta funcionalidade permite ler um c√≥digo de barras. Para tal, voc√™ precisa especificar os formatos de c√≥digo de barras que a sua aplica√ß√£o pode ler(veja a <a href="https://firebase.google.com/docs/ml-kit/android/read-barcodes?hl=pt-pt#configure-the-barcode-detector" data-href="https://firebase.google.com/docs/ml-kit/android/read-barcodes?hl=pt-pt#configure-the-barcode-detector" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">lista completa de formatos aqui</a>), atrav√©s de um objecto <code class="markup--code markup--p-code">FirebaseVisionBarcodeDetectorOptions</code>:</p><pre name="d474" id="d474" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionBarcodeDetectorOptions options =<br>    new FirebaseVisionBarcodeDetectorOptions.Builder()<br>        .setBarcodeFormats(FirebaseVisionBarcode.FORMAT_QR_CODE,<br>                           FirebaseVisionBarcode.FORMAT_AZTEC)<br>        .build();</code></pre><p name="e9fe" id="e9fe" class="graf graf--p graf-after--pre">De seguida, o processo √© o mesmo: usar o Detector, adicionar um listener e receber o c√≥digo de barras.</p><pre name="5383" id="5383" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionBarcodeDetector detector = FirebaseVision.getInstance()<br>    .getVisionBarcodeDetector(options);</code></pre><p name="0c5a" id="0c5a" class="graf graf--p graf-after--pre">Encontre o c√≥digo completo no fim do artigo.</p><h4 name="cfe9" id="cfe9" class="graf graf--h4 graf-after--p">Dete√ß√£o Facial</h4><p name="2703" id="2703" class="graf graf--p graf-after--h4">A dete√ß√£o facial √© capaz de encontrar faces humanas numa imagem e identificar caracter√≠sticas como:</p><ul class="postList"><li name="33ed" id="33ed" class="graf graf--li graf-after--p">√¢ngulo da face;</li><li name="5a20" id="5a20" class="graf graf--li graf-after--li">pontos de interesse‚Ää‚Äî‚Ääolhos(o olho esquerdo √© distinguido do direito), boca, nariz, etc.</li><li name="3803" id="3803" class="graf graf--li graf-after--li">caracter√≠sticas‚Ää‚Äî‚Äädeteta se a pessoa est√° a sorrir, est√° com os olhos abertos, etc.</li></ul><p name="4503" id="4503" class="graf graf--p graf-after--li">E voc√™ j√° conhece o processo:</p><pre name="3b69" id="3b69" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionFaceDetector detector = FirebaseVision.getInstance()<br>    .getVisionFaceDetector();</code></pre><p name="5eb0" id="5eb0" class="graf graf--p graf-after--pre">Para esta experi√™ncia eu utilizei os valores que v√™m definidos no exemplo dado na documenta√ß√£o do Firebase. Mas provavelmente irei escrever um artigo com mais detalhes sobre a dete√ß√£o facial.</p><h4 name="8969" id="8969" class="graf graf--h4 graf-after--p">Legendas para¬†imagens</h4><p name="1a02" id="1a02" class="graf graf--p graf-after--h4">Esta funcionalidade reconhece entidades (legendas/r√≥tulos) numa imagem. Por exemplo, na imagem abaixo, ela pode reconhecer as legendas: est√°dio, desporto, evento, lazer, futebol, rede, planta,etc.</p><figure name="3199" id="3199" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*78RCU2PiVq5Efkd1yGGxSw.jpeg" data-width="1024" data-height="678" src="https://cdn-images-1.medium.com/max/800/1*78RCU2PiVq5Efkd1yGGxSw.jpeg"></figure><p name="ccbd" id="ccbd" class="graf graf--p graf-after--figure">O ML Kit √© capaz de reconhecer mais de 400 entidades, incluindo: pessoas(aglomerado de gente, selfie, sorriso), atividades (a dan√ßar, a comer, a surfar), coisas(um carro, um piano, uma receita), animais(p√°ssaros, gatos, c√£es), plantas(flores, frutas, vegetais) ou at√© locais (praias, lagos, montanhas).</p><p name="7ae5" id="7ae5" class="graf graf--p graf-after--p">Para come√ßar, temos de adicionar mais uma depend√™ncia ao gradle:</p><pre name="29b7" id="29b7" class="graf graf--pre graf-after--p">implementation &#39;com.google.firebase:firebase-ml-vision-image-label-model:15.0.0&#39;</pre><p name="9f5f" id="9f5f" class="graf graf--p graf-after--pre">E o nosso detector √©:</p><pre name="f04c" id="f04c" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionLabelDetector detector = FirebaseVision.getInstance()<br>    .getVisionLabelDetector();</code></pre><p name="c47f" id="c47f" class="graf graf--p graf-after--pre">Criei uma Activity que usa todos os detetores em simult√¢neo para poder test√°-los. Se voc√™ quiser experimentar tamb√©m, pode encontrar a Activity <a href="https://gist.github.com/rosariopfernandes/603bb860f7b1ccd82759ba8ce36e52b2" data-href="https://gist.github.com/rosariopfernandes/603bb860f7b1ccd82759ba8ce36e52b2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">neste gist</a>.</p><h3 name="b052" id="b052" class="graf graf--h3 graf-after--p">Custom Models</h3><p name="5349" id="5349" class="graf graf--p graf-after--h3">Quem j√° tem experi√™ncia com Machine Learning pode utilizar os seus pr√≥prios modelos em conjunto com o ML Kit. Crie os modelos utilizando o TensorFlow Lite e importe-os na sua aplica√ß√£o.</p><p name="4776" id="4776" class="graf graf--p graf-after--p graf--trailing">Por enquanto n√£o entrarei em detalhes sobre os custom models, mas se voc√™ estiver interessado/curioso, pode saber mais na <a href="https://firebase.google.com/docs/ml-kit/android/use-custom-models?hl=pt-pt" data-href="https://firebase.google.com/docs/ml-kit/android/use-custom-models?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documenta√ß√£o</a>.</p></div></div></section><section name="5a2e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4b8f" id="4b8f" class="graf graf--p graf--leading">Era de se esperar que o Firebase estivesse a preparar uma super novidade para o Google I/O 2018. O ML Kit √© certamente bem vindo. De certeza que ir√° ajudar v√°rios desenvolvedores a criarem aplica√ß√µes melhores.</p><p name="8682" id="8682" class="graf graf--p graf-after--p">A facilidade de uso √© uma das principais vantagens. Apesar da componente Cloud ser paga, a componente gr√°tis (on-device) √© bastante poderosa e √∫til.</p><p name="08d8" id="08d8" class="graf graf--p graf-after--p graf--trailing">Eu sempre quis experimentar a Cloud Vision API, mas o facto dela exigir um cart√£o de cr√©dito sempre me fez hesitar. Mas agora que existe o ML Kit, posso por em pr√°tica todas as ideias que tenho tido para projetos futuros. Irei partilhar algumas delas aqui no Medium, ent√£o continue me seguindo para n√£o perd√™-las.</p></div></div></section><section name="2f75" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e2d1" id="e2d1" class="graf graf--p graf--leading graf--trailing">Caso tenha alguma d√∫vida ou sugest√£o, deixe abaixo nos coment√°rios. Se voc√™ estiver tentando usar o ML Kit e teve um problema, <a href="https://pt.stackoverflow.com/questions/ask?tags=firebase" data-href="https://pt.stackoverflow.com/questions/ask?tags=firebase" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">coloque ele no StackOverflow</a>, explicando o que voc√™ fez e qual foi o erro que teve. De certeza que voc√™ obter√° ajuda de mim ou de algu√©m da comunidade. üôÇ</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@thatfire.dev" class="p-author h-card">Ros√°rio Pereira Fernandes</a> on <a href="https://medium.com/p/540f8e5438c1"><time class="dt-published" datetime="2018-05-09T06:01:01.616Z">May 9, 2018</time></a>.</p><p><a href="https://medium.com/@thatfire.dev/mlkit-540f8e5438c1" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 24, 2025.</p></footer></article></body></html>